import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns  # å¼•å…¥ seaborn ç”»æ›´å¥½çœ‹çš„å›¾
import os
import copy

# ==========================================
# 0. ç¯å¢ƒé…ç½®
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"PyTorch Version: {torch.__version__}")
print(f"Running on: {DEVICE}")


def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True


set_seed(42)


# ==========================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (å¢åŠ éªŒè¯é›†åˆ’åˆ†)
# ==========================================
def load_and_process_data(file_path):
    print(f"æ­£åœ¨è¯»å– CSV æ•°æ®: {file_path} ...")

    required_cols = [
        'soc', 'totalvoltage', 'totalcurrent',
        'minvoltagebattery', 'maxvoltagebattery',
        'mintemperaturevalue', 'maxtemperaturevalue'
    ]

    try:
        df = pd.read_csv(file_path, usecols=lambda c: c in required_cols)
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None, None

    df = df.dropna()
    df = df[(df['soc'] >= 0) & (df['soc'] <= 100)]

    target_col = 'soc'
    feature_cols = [c for c in df.columns if c != target_col]

    X = df[feature_cols].values
    y = df[target_col].values.reshape(-1, 1)

    # å½’ä¸€åŒ– SOC åˆ° 0-1
    y = y / 100.0

    print(f"æ•°æ®æ€»é‡: {len(df)}")
    return X, y


# ==========================================
# 2. å®šä¹‰ MLP æ¨¡å‹
# ==========================================
class BatterySOC_MLP(nn.Module):
    def __init__(self, input_dim):
        super(BatterySOC_MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),  # ç¨å¾®å¢åŠ  Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ

            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),

            nn.Linear(64, 32),
            nn.ReLU(),

            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)


# ==========================================
# 3. æ—©åœæœºåˆ¶ç±» (æ–°å¢)
# ==========================================
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        """
        patience: å®¹å¿å¤šå°‘ä¸ª epoch éªŒè¯é›† loss æ²¡æœ‰ä¸‹é™
        min_delta: è§†ä¸ºæ”¹è¿›çš„æœ€å°å˜åŒ–é‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model_wts = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_model_wts = copy.deepcopy(model.state_dict())
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.best_model_wts = copy.deepcopy(model.state_dict())
            self.counter = 0


# ==========================================
# 4. è®­ç»ƒä¸è¯„ä¼°ä¸»ç¨‹åº
# ==========================================
def main():
    csv_file = 'vin93.csv'
    if not os.path.exists(csv_file):
        create_dummy_csv(csv_file)

    X, y = load_and_process_data(csv_file)
    if X is None: return

    # --- 1. ç§‘å­¦çš„æ•°æ®é›†åˆ’åˆ† (Train/Val/Test) ---
    # ç¬¬ä¸€æ¬¡åˆ’åˆ†ï¼šTrain+Val (80%) å’Œ Test (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    # ç¬¬äºŒæ¬¡åˆ’åˆ†ï¼šTrain (80% of 80% = 64%) å’Œ Val (20% of 80% = 16%)
    # è¿™é‡Œå®é™…ä¸Šæ˜¯ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡º 20% åšéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)

    print(f"æ•°æ®é›†åˆ’åˆ† -> è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}, æµ‹è¯•é›†: {len(X_test)}")

    # --- 2. æ ‡å‡†åŒ– (å¿…é¡»åªåœ¨è®­ç»ƒé›†ä¸Š fit) ---
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)  # ç”¨è®­ç»ƒé›†çš„å‚æ•°è½¬æ¢éªŒè¯é›†
    X_test_scaled = scaler.transform(X_test)  # ç”¨è®­ç»ƒé›†çš„å‚æ•°è½¬æ¢æµ‹è¯•é›†

    # è½¬ Tensor
    train_ds = TensorDataset(torch.FloatTensor(X_train_scaled).to(DEVICE), torch.FloatTensor(y_train).to(DEVICE))
    val_ds = TensorDataset(torch.FloatTensor(X_val_scaled).to(DEVICE), torch.FloatTensor(y_val).to(DEVICE))
    test_ds = TensorDataset(torch.FloatTensor(X_test_scaled).to(DEVICE), torch.FloatTensor(y_test).to(DEVICE))

    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)
    # Test loader ä¸éœ€è¦ shuffle
    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶ ---
    model = BatterySOC_MLP(input_dim=X.shape[1]).to(DEVICE)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # [æ–°å¢] å­¦ä¹ ç‡è°ƒåº¦å™¨: å½“éªŒè¯é›† loss ä¸ä¸‹é™æ—¶ï¼Œå°†å­¦ä¹ ç‡ä¹˜ä»¥ 0.5
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    # [æ–°å¢] æ—©åœå¯¹è±¡
    early_stopping = EarlyStopping(patience=15, min_delta=0.00001)

    # --- 4. è®­ç»ƒå¾ªç¯ (å¸¦éªŒè¯) ---
    epochs = 200
    history = {'train_loss': [], 'val_loss': []}

    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * inputs.size(0)
        train_loss /= len(train_loader.dataset)

        # éªŒè¯é˜¶æ®µ (è¿™æ˜¯è®©å®éªŒæœ‰è¯´æœåŠ›çš„å…³é”®)
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item() * inputs.size(0)
        val_loss /= len(val_loader.dataset)

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)

        # æ£€æŸ¥æ—©åœ
        early_stopping(val_loss, model)

        if (epoch + 1) % 10 == 0 or early_stopping.early_stop:
            print(f"Epoch [{epoch + 1}/{epochs}] | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")

        if early_stopping.early_stop:
            print("â¹ï¸ æ—©åœè§¦å‘ï¼åœæ­¢è®­ç»ƒã€‚")
            break

    # åŠ è½½è¡¨ç°æœ€å¥½çš„æ¨¡å‹æƒé‡ (è€Œä¸æ˜¯æœ€åä¸€ä¸ª epoch çš„æƒé‡)
    print("æ­£åœ¨åŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹æƒé‡...")
    model.load_state_dict(early_stopping.best_model_wts)

    # --- 5. æœ€ç»ˆæµ‹è¯• (Unbiased Evaluation) ---
    print("\næ­£åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½...")
    model.eval()
    y_preds = []
    y_trues = []

    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            y_preds.append(outputs.cpu().numpy())
            y_trues.append(targets.cpu().numpy())

    y_pred = np.vstack(y_preds)
    y_true = np.vstack(y_trues)

    # è¿˜åŸç™¾åˆ†æ¯”
    y_pred_perc = y_pred * 100
    y_true_perc = y_true * 100

    # è®¡ç®—æŒ‡æ ‡
    rmse = np.sqrt(mean_squared_error(y_true_perc, y_pred_perc))
    mae = mean_absolute_error(y_true_perc, y_pred_perc)
    r2 = r2_score(y_true_perc, y_pred_perc)

    print("=" * 40)
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•é›†ç»“æœ (Test Set):")
    print(f"RMSE : {rmse:.4f} %")
    print(f"MAE  : {mae:.4f} %")
    print(f"R2   : {r2:.4f}")
    print("=" * 40)

    # --- 6. é«˜çº§ç»˜å›¾åˆ†æ ---
    plot_results(history, y_true_perc, y_pred_perc)


def plot_results(history, y_true, y_pred):
    plt.figure(figsize=(18, 5))

    # å›¾1: æŸå¤±æ›²çº¿å¯¹æ¯” (Train vs Val)
    # è¿™é‡Œçš„è¯´æœåŠ›åœ¨äºï¼šè¯æ˜äº†æ¨¡å‹æ²¡æœ‰ä¸¥é‡çš„è¿‡æ‹Ÿåˆï¼ˆTrainå’ŒValæ›²çº¿åº”è¯¥è´´åˆç´§å¯†ï¼‰
    plt.subplot(1, 3, 1)
    plt.plot(history['train_loss'], label='Train Loss', color='blue')
    plt.plot(history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Loss Curve (Train vs Validation)')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å›¾2: å›å½’æ‹Ÿåˆå›¾ (Predicted vs True)
    # è¿™é‡Œçš„è¯´æœåŠ›åœ¨äºï¼šç‚¹è¶Šé è¿‘å¯¹è§’çº¿ï¼Œè¯´æ˜é¢„æµ‹è¶Šå‡†
    plt.subplot(1, 3, 2)
    plt.scatter(y_true, y_pred, alpha=0.3, s=10, color='green')
    min_val, max_val = 0, 100
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal 1:1')
    plt.title(f'Regression: R2={r2_score(y_true, y_pred):.4f}')
    plt.xlabel('True SOC (%)')
    plt.ylabel('Predicted SOC (%)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å›¾3: è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾ (Error Histogram)
    # è¿™é‡Œçš„è¯´æœåŠ›åœ¨äºï¼šè¯æ˜è¯¯å·®æ˜¯é›¶å‡å€¼çš„æ­£æ€åˆ†å¸ƒï¼Œè€Œä¸æ˜¯æœ‰ç³»ç»Ÿæ€§åå·®
    plt.subplot(1, 3, 3)
    errors = y_pred - y_true
    sns.histplot(errors, bins=50, kde=True, color='purple')
    plt.title('Error Distribution (Pred - True)')
    plt.xlabel('SOC Error (%)')
    plt.ylabel('Frequency')
    plt.axvline(x=0, color='r', linestyle='--')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = 'soc_analysis_report.png'
    plt.savefig(save_path, dpi=300)
    print(f"\nğŸ“Š è¯¦ç»†åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")


# æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
def create_dummy_csv(filename):
    print(f"æœªæ‰¾åˆ° {filename}ï¼Œæ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    rows = 5000
    data = {
        'soc': np.random.uniform(0, 100, rows),
        'totalvoltage': np.random.uniform(300, 400, rows),
        'totalcurrent': np.random.normal(0, 50, rows),
        'minvoltagebattery': np.random.uniform(3.0, 4.2, rows),
        'maxvoltagebattery': np.random.uniform(3.0, 4.2, rows),
        'mintemperaturevalue': np.random.uniform(20, 40, rows),
        'maxtemperaturevalue': np.random.uniform(22, 45, rows)
    }
    pd.DataFrame(data).to_csv(filename, index=False)


if __name__ == "__main__":
    main()